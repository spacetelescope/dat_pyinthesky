{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "# MIRI MRS Spectroscopy of a Late M Star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use case:** Extract spatial-spectral features from IFU cube and measure their attributes.<br>\n",
    "**Data:** KMOS datacube of point sources in the LMC from Jones et al. (in prep).<br>\n",
    "**Tools:** specutils, spectral_cube, photutils, astropy, aplpy, scipy.<br>\n",
    "**Cross-intrument:** MIRI<br>\n",
    "**Documentation:** This notebook is part of a STScI's larger [post-pipeline Data Analysis Tools Ecosystem](https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis).<br>\n",
    "\n",
    "**Note**: Ultimately, this notebook will include MIRI simulated data cubes obtained using MIRISim (https://wiki.miricle.org//bin/view/Public/MIRISim_Public)\n",
    "and run through the JWST pipeline (https://jwst-pipeline.readthedocs.io/en/latest/) of\n",
    "point sources with spectra representative of late M type stars.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook analyzes one star represented by a dusty SED corresponding to the ISO SWS spectrum of\n",
    "W Per from Kraemer et al. (2002) and Sloan et al. (2003) to cover the MRS spectral range 5-28 microns.  Analysis of JWST spectral cubes requires extracting spatial-spectral features of interest and measuring their attributes. \n",
    "\n",
    "The first part of the notebook will process the datacube and automatically detect and extract spectra (summed over its spatial region) for all point sources in the cube.  Then it will read in a datacube generated at Stage 3 of the JWST pipeline or use near-IR data from KMOS as a representative example of an IR data cube.  The analysis will use `photutils` to automatically detect sources in the continuum image and use an aperture mask generated with `spectral-cube` to extract the spectra of each point source in the data cube.\n",
    "\n",
    "The second part of the notebook will perform data analysis using `specutils`.  Specifically, it will fit a model photosphere/blackbody to the spectra.  Then it will calculate the centroids, line integrated flux and equivalent width for each dust and molecular feature. \n",
    "\n",
    "## To Do:\n",
    "- Replace KMOS data cube with JWST/MIRI simulation of an M star ran through JWST piplieline.\n",
    "- Make function to extract spectra from datacube using an apeture.\n",
    "- Replace blackbody fit to the photosphere part of the spectra with a stellar photosphere model.\n",
    "- Make sure errors have been propagated correctly in the caculation of centroids, line integrated flux and\n",
    "equivalent widths.\n",
    "- Make simple function within the `specutils` framework to fit a continium and measure centroids, line integrated flux and\n",
    "equivalent widths of broad solid state and molecular features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import useful python packages\n",
    "import numpy as np\n",
    "\n",
    "# Import packages to display images inline in the notebook\n",
    "import matplotlib.pyplot as plt    \n",
    "%matplotlib inline   \n",
    "\n",
    "# Set general plotting options\n",
    "params={'legend.fontsize':'18','axes.labelsize':'18',\n",
    "        'axes.titlesize':'18','xtick.labelsize':'18',\n",
    "        'ytick.labelsize':'18','lines.linewidth':2,'axes.linewidth':2,'animation.html': 'html5'}\n",
    "plt.rcParams.update(params)\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import astropy packages \n",
    "from astropy import units as u\n",
    "from astropy.io import ascii\n",
    "from astropy.wcs import WCS\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.nddata import StdDevUncertainty\n",
    "from astropy.io import fits # added by BAS on 8 April 2021\n",
    "\n",
    "# Import packages to deal with spectralcubes\n",
    "from spectral_cube import SpectralCube\n",
    "\n",
    "# To find stars in the MRS spectralcubes and do aperture photometry\n",
    "from photutils import DAOStarFinder, CircularAperture\n",
    "\n",
    "# To deal with 1D spectrum\n",
    "from specutils import Spectrum1D\n",
    "from specutils.fitting import fit_generic_continuum\n",
    "from specutils.manipulation import box_smooth, extract_region, SplineInterpolatedResampler\n",
    "from specutils.analysis import line_flux, centroid, equivalent_width\n",
    "from specutils.spectra import SpectralRegion\n",
    "\n",
    "# To make nice plots with WCS axis\n",
    "import aplpy\n",
    "\n",
    "# To fit a curve to the data\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths to the Data and Outputs\n",
    "\n",
    "For now use KMOS data cube of YSOs in the LMC from Jones et al in prep.\n",
    "\n",
    "TODO: Update with MIRISim JWST pipeline processed data in future itterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other stuff\n",
    "\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "import json\n",
    "import glob\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.associations import asn_from_list\n",
    "import crds\n",
    "from jdaviz.app import Application\n",
    "import asdf\n",
    "from photutils import aperture_photometry\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do Stage 1 pipeline\n",
    "\n",
    "allshortfiles=glob.glob('/filepath/and/filenames_of_the_MIRIFUSHORT_mirisim_simulation_files_here.fits')\n",
    "alllongfiles=glob.glob('/filepath/and/filenames_of_the_MIRIFULONG_mirisim_simulation_files_here.fits')\n",
    "\n",
    "pipe1short = Detector1Pipeline()\n",
    "\n",
    "# run calwebb_detector1 on the MIRIFUSHORT data separate from MIRIFULONG data, as it saves time this way\n",
    "for shortfile in allshortfiles:\n",
    "    print(shortfile)\n",
    "    baseshort,remaindershort = shortfile.split('.')\n",
    "    \n",
    "    beforestuffshort,dateafterstuffshort = shortfile.split('2021_')\n",
    "    #Since there are 3 simulations folders, each corresponding to SHORT, MEDIUM, and LONG band mirisim\n",
    "    #  simulations, we need a way to identify the pipeline's products resulting from these input simulations,  \n",
    "    #  so we identify based on the number string in the folder name giving month and day and time of  \n",
    "    #  simulation creation.  There may be a better way to do this, but this works for now.\n",
    "    datestringshort,afterstuffshort = dateafterstuffshort.split('_mirisim')\n",
    "    \n",
    "    pipe1short.refpix.skip = True\n",
    "    pipe1short.output_file = baseshort+datestringshort\n",
    "    \n",
    "    pipe1short.run(shortfile)\n",
    "\n",
    "pipe1long = Detector1Pipeline()\n",
    "\n",
    "for longfile in alllongfiles:\n",
    "    print(longfile)\n",
    "    baselong,remainderlong = longfile.split('.')\n",
    "    \n",
    "    beforestufflong,dateafterstufflong = longfile.split('2021_')\n",
    "    #same as above\n",
    "    datestringlong,afterstufflong = dateafterstufflong.split('_mirisim')\n",
    "    \n",
    "    pipe1long.refpix.skip = True\n",
    "    pipe1long.output_file = baselong+datestringlong\n",
    "    \n",
    "    pipe1long.run(longfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do Stage 2 pipeline\n",
    "\n",
    "allshortfiles2 = glob.glob('det_image_*_MIRIFUSHORT_*_rate.fits')\n",
    "alllongfiles2 = glob.glob('det_image_*_MIRIFULONG_*_rate.fits')\n",
    "\n",
    "for short2file in allshortfiles2:\n",
    "    print(short2file)\n",
    "    pipe2short = Spec2Pipeline()\n",
    "    base2short,remainder2short = short2file.split('.')\n",
    "    \n",
    "    pipe2short.straylight.skip = True# skip stray light step\n",
    "    pipe2short.cube_build.skip = True# skip cube building step (for now, but see my comment in next cell)\n",
    "    pipe2short.extract_1d.skip = True# skip extract_1d step\n",
    "    pipe2short.output_file = base2short\n",
    "    \n",
    "    pipe2short.run(short2file)\n",
    "\n",
    "for long2file in alllongfiles2:\n",
    "    print(long2file)\n",
    "    pipe2long = Spec2Pipeline()\n",
    "    base2long,remainder2long = long2file.split('.')\n",
    "    \n",
    "    pipe2long.straylight.skip = True\n",
    "    pipe2long.cube_build.skip = True\n",
    "    pipe2long.extract_1d.skip = True\n",
    "    pipe2long.output_file = base2long\n",
    "    \n",
    "    pipe2long.run(long2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Sargent, 10 May 2021\n",
    "# The first time you run this notebook, skip this cell and proceed to the next cell.\n",
    "#   Then, when you have a cube produced from stage 3 of the pipeline, open the cube \n",
    "#   outside of this notebook and find the RA and Dec of the point source in the cube.  \n",
    "#   Come back to this cell and set those RA and Dec equal to targra and targdec in this \n",
    "#   cell, respectively, run this cell, then re-do the next 2 cells to re-do the stage 3 \n",
    "#   pipeline reduction.\n",
    "#\n",
    "# In the future, perhaps that awkward process could be replaced.  Specifically, I am \n",
    "#   thinking that, further down in this notebook, Libby has photutils code that can \n",
    "#   identify a point source in the data.  That could be done here on a single cube\n",
    "#   produced by the preceding cell that does the stage 2 reduction, and the pixel \n",
    "#   coordinates returned could be used to compute the RA and Dec of the source, then \n",
    "#   that RA and Dec could be used here instead of having to open the stage 3 cube \n",
    "#   outside of this notebook to get the source's RA and Dec like is currently needed.\n",
    "\n",
    "all_files = glob.glob('det_image_*_cal.fits')\n",
    "targra=# Put the RA you find from the cube you get the first time you run the stage 3 pipeline here.\n",
    "targdec=# Put the Dec you find from the cube you get the first time you run the stage 3 pipeline here.\n",
    "for thisfile in all_files:\n",
    "    base,remainder = thisfile.split('.')\n",
    "    outfilename=base+'_fix.'+remainder\n",
    "    print(outfilename)\n",
    "    \n",
    "    with fits.open(thisfile) as hduthis:\n",
    "        hduthis['SCI'].header['SRCTYPE']='POINT'# This tells the extract_1d step to extract a point source.\n",
    "        hduthis[0].header['TARG_RA']=targra\n",
    "        hduthis[0].header['TARG_DEC']=targdec\n",
    "        hduthis.writeto(outfilename,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up needed reference file for stage 3\n",
    "\n",
    "file_all_list = glob.glob('det_image_*_cal_fix.fits')\n",
    "\n",
    "asnall = asn_from_list.asn_from_list(file_all_list, rule=DMS_Level3_Base,product_name='combine_dithers_all_exposures')\n",
    "\n",
    "asnallfile='for_spec3_all.json'\n",
    "with open(asnallfile, 'w') as fpall:\n",
    "    fpall.write(asnall.dump()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do stage 3 pipeline\n",
    "\n",
    "pipe3ss = Spec3Pipeline()\n",
    "pipe3ss.master_background.skip = True\n",
    "pipe3ss.mrs_imatch.skip = True\n",
    "pipe3ss.outlier_detection.skip = True\n",
    "pipe3ss.resample_spec.skip = True\n",
    "pipe3ss.combine_1d.skip = True\n",
    "#pipe3ss.cube_build.output_type='multi'# un-comment this if you want a cube spanning the entire MRS wavelength range\n",
    "pipe3ss.use_source_posn='True'\n",
    "pipe3ss.subtract_background='True'\n",
    "#pipe3ss.extract_1d.apply_apcorr = False# un-comment this if you do not want the aperture corrction applied\n",
    "pipe3ss.output_file = 'allspec3'\n",
    "pipe3ss.run(asnallfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlall=[]\n",
    "fnuall=[]\n",
    "dfnuall=[]\n",
    "bandall=[]\n",
    "\n",
    "speclist=[]\n",
    "\n",
    "allch_extractfile='combine_dithers_all_exposures_ch1-2-3-4-mediumlongshort-_x1d.fits'\n",
    "with fits.open(allch_extractfile) as hduallch:\n",
    "    wlallch=hduallch['EXTRACT1D'].data['wavelength']\n",
    "    fnuallch=hduallch['EXTRACT1D'].data['flux']\n",
    "    dfnuallch=hduallch['EXTRACT1D'].data['error']\n",
    "\n",
    "ch1short_extractfile='combine_dithers_all_exposures_ch1-short_x1d.fits'\n",
    "with fits.open(ch1short_extractfile) as hduch1short:\n",
    "    wlch1short=hduch1short['EXTRACT1D'].data['wavelength']\n",
    "    fnuch1short=hduch1short['EXTRACT1D'].data['flux']\n",
    "    dfnuch1short=hduch1short['EXTRACT1D'].data['error']\n",
    "    print(dfnuch1short)\n",
    "ch1shortspec=Spectrum1D(spectral_axis=wlch1short*u.micron,flux=fnuch1short*u.Jy,uncertainty=dfnuch1short,meta={'band':'ch1short'})\n",
    "speclist.append(ch1shortspec)\n",
    "\n",
    "ch1medium_extractfile='combine_dithers_all_exposures_ch1-medium_x1d.fits'\n",
    "with fits.open(ch1medium_extractfile) as hduch1medium:\n",
    "    wlch1medium=hduch1medium['EXTRACT1D'].data['wavelength']\n",
    "    fnuch1medium=hduch1medium['EXTRACT1D'].data['flux']\n",
    "    dfnuch1medium=hduch1medium['EXTRACT1D'].data['error']\n",
    "ch1mediumspec=Spectrum1D(spectral_axis=wlch1medium*u.micron,flux=fnuch1medium*u.Jy,uncertainty=dfnuch1medium,meta={'band':'ch1medium'})\n",
    "speclist.append(ch1mediumspec)\n",
    "\n",
    "ch1long_extractfile='combine_dithers_all_exposures_ch1-long_x1d.fits'\n",
    "with fits.open(ch1long_extractfile) as hduch1long:\n",
    "    wlch1long=hduch1long['EXTRACT1D'].data['wavelength']\n",
    "    fnuch1long=hduch1long['EXTRACT1D'].data['flux']\n",
    "    dfnuch1long=hduch1long['EXTRACT1D'].data['error']\n",
    "ch1longspec=Spectrum1D(spectral_axis=wlch1long*u.micron,flux=fnuch1long*u.Jy,uncertainty=dfnuch1long,meta={'band':'ch1long'})\n",
    "speclist.append(ch1longspec)\n",
    "\n",
    "ch2short_extractfile='combine_dithers_all_exposures_ch2-short_x1d.fits'\n",
    "with fits.open(ch2short_extractfile) as hduch2short:\n",
    "    wlch2short=hduch2short['EXTRACT1D'].data['wavelength']\n",
    "    fnuch2short=hduch2short['EXTRACT1D'].data['flux']\n",
    "    dfnuch2short=hduch2short['EXTRACT1D'].data['error']\n",
    "ch2shortspec=Spectrum1D(spectral_axis=wlch2short*u.micron,flux=fnuch2short*u.Jy,uncertainty=dfnuch2short,meta={'band':'ch2short'})\n",
    "speclist.append(ch2shortspec)\n",
    "\n",
    "ch2medium_extractfile='combine_dithers_all_exposures_ch2-medium_x1d.fits'\n",
    "with fits.open(ch2medium_extractfile) as hduch2medium:\n",
    "    wlch2medium=hduch2medium['EXTRACT1D'].data['wavelength']\n",
    "    fnuch2medium=hduch2medium['EXTRACT1D'].data['flux']\n",
    "    dfnuch2medium=hduch2medium['EXTRACT1D'].data['error']\n",
    "ch2mediumspec=Spectrum1D(spectral_axis=wlch2medium*u.micron,flux=fnuch2medium*u.Jy,uncertainty=dfnuch2medium,meta={'band':'ch2medium'})\n",
    "speclist.append(ch2mediumspec)\n",
    "\n",
    "ch2long_extractfile='combine_dithers_all_exposures_ch2-long_x1d.fits'\n",
    "with fits.open(ch2long_extractfile) as hduch2long:\n",
    "    wlch2long=hduch2long['EXTRACT1D'].data['wavelength']\n",
    "    fnuch2long=hduch2long['EXTRACT1D'].data['flux']\n",
    "    dfnuch2long=hduch2long['EXTRACT1D'].data['error']\n",
    "ch2longspec=Spectrum1D(spectral_axis=wlch2long*u.micron,flux=fnuch2long*u.Jy,uncertainty=dfnuch2long,meta={'band':'ch2long'})\n",
    "speclist.append(ch2longspec)\n",
    "\n",
    "ch3short_extractfile='combine_dithers_all_exposures_ch3-short_x1d.fits'\n",
    "with fits.open(ch3short_extractfile) as hduch3short:\n",
    "    wlch3short=hduch3short['EXTRACT1D'].data['wavelength']\n",
    "    fnuch3short=hduch3short['EXTRACT1D'].data['flux']\n",
    "    dfnuch3short=hduch3short['EXTRACT1D'].data['error']\n",
    "ch3shortspec=Spectrum1D(spectral_axis=wlch3short*u.micron,flux=fnuch3short*u.Jy,uncertainty=dfnuch3short,meta={'band':'ch3short'})\n",
    "speclist.append(ch3shortspec)\n",
    "\n",
    "ch3medium_extractfile='combine_dithers_all_exposures_ch3-medium_x1d.fits'\n",
    "with fits.open(ch3medium_extractfile) as hduch3medium:\n",
    "    wlch3medium=hduch3medium['EXTRACT1D'].data['wavelength']\n",
    "    fnuch3medium=hduch3medium['EXTRACT1D'].data['flux']\n",
    "    dfnuch3medium=hduch3medium['EXTRACT1D'].data['error']\n",
    "ch3mediumspec=Spectrum1D(spectral_axis=wlch3medium*u.micron,flux=fnuch3medium*u.Jy,uncertainty=dfnuch3medium,meta={'band':'ch3medium'})\n",
    "speclist.append(ch3mediumspec)\n",
    "\n",
    "ch3long_extractfile='combine_dithers_all_exposures_ch3-long_x1d.fits'\n",
    "with fits.open(ch3long_extractfile) as hduch3long:\n",
    "    wlch3long=hduch3long['EXTRACT1D'].data['wavelength']\n",
    "    fnuch3long=hduch3long['EXTRACT1D'].data['flux']\n",
    "    dfnuch3long=hduch3long['EXTRACT1D'].data['error']\n",
    "ch3longspec=Spectrum1D(spectral_axis=wlch3long*u.micron,flux=fnuch3long*u.Jy,uncertainty=dfnuch3long,meta={'band':'ch3long'})\n",
    "speclist.append(ch3longspec)\n",
    "\n",
    "ch4short_extractfile='combine_dithers_all_exposures_ch4-short_x1d.fits'\n",
    "with fits.open(ch4short_extractfile) as hduch4short:\n",
    "    wlch4short=hduch4short['EXTRACT1D'].data['wavelength']\n",
    "    fnuch4short=hduch4short['EXTRACT1D'].data['flux']\n",
    "    dfnuch4short=hduch4short['EXTRACT1D'].data['error']\n",
    "ch4shortspec=Spectrum1D(spectral_axis=wlch4short*u.micron,flux=fnuch4short*u.Jy,uncertainty=dfnuch4short,meta={'band':'ch4short'})\n",
    "speclist.append(ch4shortspec)\n",
    "\n",
    "ch4medium_extractfile='combine_dithers_all_exposures_ch4-medium_x1d.fits'\n",
    "with fits.open(ch4medium_extractfile) as hduch4medium:\n",
    "    wlch4medium=hduch4medium['EXTRACT1D'].data['wavelength']\n",
    "    fnuch4medium=hduch4medium['EXTRACT1D'].data['flux']\n",
    "    dfnuch4medium=hduch4medium['EXTRACT1D'].data['error']\n",
    "ch4mediumspec=Spectrum1D(spectral_axis=wlch4medium*u.micron,flux=fnuch4medium*u.Jy,uncertainty=dfnuch4medium,meta={'band':'ch4medium'})\n",
    "speclist.append(ch4mediumspec)\n",
    "\n",
    "ch4long_extractfile='combine_dithers_all_exposures_ch4-long_x1d.fits'\n",
    "with fits.open(ch4long_extractfile) as hduch4long:\n",
    "    wlch4long=hduch4long['EXTRACT1D'].data['wavelength']\n",
    "    fnuch4long=hduch4long['EXTRACT1D'].data['flux']\n",
    "    dfnuch4long=hduch4long['EXTRACT1D'].data['error']\n",
    "ch4longspec=Spectrum1D(spectral_axis=wlch4long*u.micron,flux=fnuch4long*u.Jy,uncertainty=dfnuch4long,meta={'band':'ch4long'})\n",
    "speclist.append(ch4longspec)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(wlallch,fnuallch,'.',color='black',markersize=1)\n",
    "plt.plot(wlch1short,fnuch1short,'.',color='purple',markersize=1)\n",
    "plt.plot(wlch1medium,fnuch1medium,'.',color='blue',markersize=1)\n",
    "plt.plot(wlch1long,fnuch1long,'.',color='green',markersize=1)\n",
    "plt.plot(wlch2short,fnuch2short,'.',color='yellow',markersize=1)\n",
    "plt.plot(wlch2medium,fnuch2medium,'.',color='orange',markersize=1)\n",
    "plt.plot(wlch2long,fnuch2long,'.',color='red',markersize=1)\n",
    "plt.plot(wlch3short,fnuch3short,'.',color='purple',markersize=1)\n",
    "plt.plot(wlch3medium,fnuch3medium,'.',color='blue',markersize=1)\n",
    "plt.plot(wlch3long,fnuch3long,'.',color='green',markersize=1)\n",
    "plt.plot(wlch4short,fnuch4short,'.',color='yellow',markersize=1)\n",
    "plt.plot(wlch4medium,fnuch4medium,'.',color='orange',markersize=1)\n",
    "plt.plot(wlch4long,fnuch4long,'.',color='red',markersize=1)\n",
    "plt.xlabel('wavelength (microns)')\n",
    "plt.ylabel('flux (Jy)')\n",
    "plt.xlim(4.0,29.0)\n",
    "plt.ylim(0.0,0.15)\n",
    "plt.title('Spectrum from extract_1d in Stage 3')\n",
    "plt.tight_layout()\n",
    "plt.savefig('spectrum.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app = Application(configuration='cubeviz')# when doing Cubeviz, first run this block.\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1short_cubefile='combine_dithers_all_exposures_ch1-long_s3d.fits'\n",
    "#ch1short_cubefile='combine_dithers_all_exposures_ch1-2-3-4-shortlongmedium-_s3d.fits'\n",
    "app.load_data(ch1short_cubefile)# When this block is run, the spectrum will appear in the Cubeviz viewer above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_viewer('spectrum-viewer').show()# This just shows the spectrum part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_input = app.get_data_from_viewer('spectrum-viewer')\n",
    "# This is the spectrum - wavelength and flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before you run this cell, draw a region (like a circle) in one of the cube viewers in Cubeviz above.\n",
    "#  Otherwise, you get an error.\n",
    "spec_agb = app.get_data_from_viewer('spectrum-viewer', 'Subset 1')\n",
    "spec_agb\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(spec_agb.spectral_axis,spec_agb.flux)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup an input directory where relevant data is located\n",
    "#data_in_path = \"https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/MRS_Spectroscopy_Late_M_Star/\"\n",
    "\n",
    "data_cube_file = 'combine_dithers_all_exposures_ch1-long_s3d.fits'\n",
    "#'combine_dithers_all_exposures_ch1-2-3-4-shortlongmedium-_s3d.fits'\n",
    "# Apparently the cell that is 2 cells from now that reads in the cube does not like the MRS cube that spans \n",
    "#   all the MRS wavelengths!  So, instead, I have here a cube for only 1 of the MRS bands, which it seems \n",
    "#   to like better.\n",
    "\n",
    "#data_in_path + \"NGC346_K_2c_COMBINED_CUBE_Y551.fits\"\n",
    "\n",
    "# Path to output directory\n",
    "data_out_path = \"./\"\n",
    "\n",
    "# Setup an output directory to save the extracted 1D spectra\n",
    "outdir_spectra = data_out_path + '/spectra/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some housekeeping if using the KMOS data rather than simulated JWST/MRS data\n",
    "# Define good wavelength ranges for each grating from which to make the data cube\n",
    "#YJgrating = [1.02, 1.358] # microns\n",
    "#Hgrating = [1.44, 1.85]   # microns\n",
    "Kgrating = [7.6, 8.6]#[2.1, 2.42]    # microns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load and Display the Data cube\n",
    "\n",
    "**Developer note**  Note the `SpectralCube` package is designed for sub-mm/radio data it expects a beam!\n",
    "This is preferred to other packages available due to much of its functionality and ease of use.\n",
    "JWST NIRSpec and MIRI both have instruments that give data cubes (with two positional dimensions and one spectral\n",
    "dimension) as the final pipeline product, as do many ground based telescopes, which do not have a beam.\n",
    "\n",
    "\n",
    "https://spectral-cube.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cube = SpectralCube.read(data_cube_file, hdu=1)  \n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cube dimensions and trimming \n",
    "# Data order in cube is (n_spectral, n_y, n_x)\n",
    "\n",
    "# Trim the ends of the cube where the data quality is poor\n",
    "subcube = cube.spectral_slab(Kgrating[0] * u.micron, Kgrating[1] * u.micron)\n",
    "\n",
    "# Rename subcube to equal cube - done in case step above is not necessary\n",
    "cube = subcube\n",
    "\n",
    "# Chop out the NaN borders\n",
    "cmin = cube.minimal_subcube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a continuum image (Sum/average over Wavelength)\n",
    "# Note: many mathematical options are available median is preferred\n",
    "cont_img = cmin[:, 10:30, 10:30].median(axis = 0)#cmin.median(axis = 0)\n",
    "\n",
    "cont_img_sub = cont_img[10:30, 10:30]\n",
    "# Extract the target name\n",
    "#name_long = cont_img.header[\"OBJECT\"]\n",
    "#name, _ = name_long.split(\"/\")\n",
    "name = 'AGB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot the continuum image now the NaN borders removed\n",
    "plt.imshow(cont_img.value)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the continuum in WCS\n",
    "F = aplpy.FITSFigure(cont_img.hdu, north = True)\n",
    "F.show_colorscale()\n",
    "F.add_label(0.1, 0.9, name, relative = True, size = 22, weight = 'bold')\n",
    "F.axis_labels.set_font(size = 22)\n",
    "F.tick_labels.set_font(size = 18, stretch = 'condensed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to detect the point source in the datacube and extract and plot the spectra for each source\n",
    "\n",
    "**Developer note** Finding a way to streamline the process of detecting sources within a data cube and extracting their\n",
    "spectra would be extremely valuable.\n",
    "\n",
    "For data cubes like the JWST/MIRI MRS information on the point sources in the FOV and also obtaining a source subtracted\n",
    " data cube will be necessary (See the `PampelMuse` software for an example on how spectral extraction is implemented for\n",
    "  near-IR data cubes like MUSE).\n",
    "\n",
    "Note these backgrounds of diffuse emission can be quite complex.\n",
    "\n",
    "On these source extracted data cubes (see `SUBTRES` in `PampelMuse`) I would like to produce moment maps\n",
    "(https://casa.nrao.edu/Release3.4.0/docs/UserMan/UserManse41.html) and Position-Velocity (PV) diagrams\n",
    "(https://casa.nrao.edu/Release4.1.0/doc/UserMan/UserManse42.html).\n",
    "\n",
    "### 1) Use `Photutils` to detect stars/point sources in the continuum image\n",
    "\n",
    "The first step of the analysis is to identify those sources for which it is feasible to extract spectra from the IFU\n",
    "data. Ideally we can estimate the signal-to-noise ratio (S/N) for all sources in the cube, do a number of checks to\n",
    "determine the status of every source and loop through these (brightest first) to extract the spectra.\n",
    "\n",
    "### 2) Extract the spectra from the datacube using `SpectralCube`\n",
    "\n",
    "**Note** There are multiple ways of extracting spectra from datacubes. The simplest is slice the cube along a single\n",
    "pixel but this is not ideal for point sources which should cover multiple pixels.\n",
    "Here I use *Aperture Extraction*. \n",
    "\n",
    "- The flux from each point source was obtained via a circular aperture. This requires you to mask the data, and make a\n",
    "circular mask and a maskedsubcube.\n",
    "\n",
    "- A background measured using a square/rectangular aperture sliced in pixel coordinates to produce a sub-cube.\n",
    "\n",
    "- A annulus surrounding the point source to measure the local background. \n",
    "\n",
    "- Using predefined regions from DS9 etc. to create a mask [`Not used here`].\n",
    "\n",
    "*If have a small number of data cubes selecting the source extraction region and background region manually using\n",
    "`cubeviz` would be useful here.*\n",
    "\n",
    "Mathematical operation e.g. `max, mean, sum, median, std` should then be applied to the region in the aperture.\n",
    "\n",
    "Below I show a few different options from the simple to the complex, which takes into account the background emission\n",
    "within the data cube. Taking into account the background may not always be the preferred method but the option should\n",
    "always be available when using an aperture extraction.\n",
    "\n",
    "#### Steps to find the background\n",
    "\n",
    "1) Define a background region either as an annulus or as a rectangle away from the source\n",
    "\n",
    "2) Find the median of all the background pixels to account for variations \n",
    "\n",
    "3) Find number of pixels in background and number of pixels in the point source aperture\n",
    "\n",
    "4) Find the sum of all the pixels in the point source aperture\n",
    "\n",
    "5) Correct for background using the sum star flux minus median of background * pixels in star aperture\n",
    "\n",
    "\n",
    "\n",
    "**Advanced Developer Note** Using Aperture Extraction to obtain the spectra for each source in the data cube is still\n",
    "very simplistic. It should be noted that the MIRI aperture changes as a function of wavelength, the steps above do not\n",
    "account for this.\n",
    "A good example of software that looks at extracting point sources from datacubes is: `PampelMuse`, by Sebastian Kamann. \n",
    "https://gitlab.gwdg.de/skamann/pampelmuse; https://ui.adsabs.harvard.edu/abs/2013A%26A...549A..71K/abstract\n",
    "\n",
    "An `optimal spectrum extraction` procedure would take into account the varying PSF through the cube, to produce an\n",
    "accurate spectra with the maximum possible signal-to-noise ratio. This weights the extracted data by the S/N of each\n",
    "pixel (Horne 1986) and would be ideal for when there is a complex background or for extracting spatially-blended source.\n",
    "For small cubes its best to fit a PSF profile to all resolved sources simultaneously, but this might not be possible in\n",
    "larger data sets.\n",
    "\n",
    "**Advanced Developer Note 2** In dense fields like globular clusters, with a significant number of unresolved sources or\n",
    " in embedded star-forming clusters, a more advanced treatment of the background would be necessary. For instance using a\n",
    " homogeneous grid across the field of view with parameters controlling the bin size would be ideal. If a variable\n",
    " background is not accounted for in a PSF extraction systematic residuals in the data would be present where background\n",
    " is over or underestimated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect, extract and plot 1D spectrum of each source in the cube \n",
    "\n",
    "### First automatically identify all the point sources in the cube using `photutils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array to store results of the source detection within the data cube\n",
    "name_val = []\n",
    "source_val = []\n",
    "ra_val =[]\n",
    "dec_val =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop out Edges and take absolute value of the continuum image\n",
    "#cont_img = cont_img[1:13, 1:13]\n",
    "\n",
    "# Find the background in the collapsed datacube\n",
    "mean, median, std = sigma_clipped_stats(cont_img.value, sigma = 2.0)\n",
    "\n",
    "# Get a list of sources using a dedicated source detection algorithm\n",
    "# Find sources at least 3* background (typically)\n",
    "\n",
    "daofind = DAOStarFinder(fwhm = 2.0, threshold = 3. * std)\n",
    "sources = daofind(cont_img.value - median) \n",
    "print(\"\\n Number of sources in field: \", len(sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If point sources are present in the cube extract and plot the spectrum of each source\n",
    "\n",
    "#### In the cell below we:\n",
    "\n",
    "1) Extract a spectra for each detected object using aperture photometry, and a circular masked region.\n",
    "\n",
    "2) Make an estimate of the background in the datacube using both: an annulus around each source and a box region away\n",
    "from the source - this box and annulus is hard coded and not ideal for other datasets or multiple cubes.\n",
    "\n",
    "3) Generate a background corrected spectrum.\n",
    "\n",
    "4) Plots the spectra and its various background corrected versions. \n",
    "\n",
    "5) Convert the spectra into Jy.\n",
    "\n",
    "6) Write each of the spectra to a file. (They could be put into a `specutils` `Spectrum1D` object at this stage but I\n",
    "have not done this here.) This file is loaded by all other routines to do analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sources) > 0:\n",
    "    print()            \n",
    "    for col in sources.colnames:\n",
    "        sources[col].info.format = '%.8g'  # for consistent table output\n",
    "\n",
    "    print(sources)                  \n",
    "\n",
    "    # From the list of sources in the field get their RA and DEC (ICRS)\n",
    "    print()\n",
    "\n",
    "    # Positions in pixels\n",
    "    positions = Table([sources['xcentroid'], sources['ycentroid']])\n",
    "\n",
    "    # Instantiate WCS object\n",
    "    w = WCS(cont_img.header)\n",
    "\n",
    "    # Convert to RA & Dec (ICRS)\n",
    "    radec_lst = w.pixel_to_world(sources['xcentroid'], sources['ycentroid'])\n",
    "    \n",
    "    #-----------------------------------------------------  \n",
    "    # We are now entering a loop which does multiple processing steps on each \n",
    "    # point source detected in the cube\n",
    "    \n",
    "    for count_s, _ in enumerate(sources):\n",
    "        print(radec_lst[count_s].to_string('hmsdms'))\n",
    "        name_val.append(name)\n",
    "        source_val.append(count_s)\n",
    "        ra_val.append(radec_lst[count_s].ra.deg)\n",
    "        dec_val.append(radec_lst[count_s].dec.deg)\n",
    "        \n",
    "        #-----------------------------------------------------           \n",
    "        # Aperture Extract spectrum of point source - using a circular aperture\n",
    "\n",
    "        # Size of frame \n",
    "        ysize_pix = cmin.shape[1]\n",
    "        xsize_pix = cmin.shape[2]\n",
    "\n",
    "        # Set up some centroid pixel for the source \n",
    "        ycent_pix = sources['ycentroid'][count_s]\n",
    "        xcent_pix = sources['xcentroid'][count_s]\n",
    "\n",
    "        # Make an aperture radius for source\n",
    "        # If made into a function this value should not be hardcoded\n",
    "        aperture_rad_pix = 2\n",
    "\n",
    "        # Make a masked array for the aperture\n",
    "        yy, xx = np.indices([ysize_pix,xsize_pix], dtype = 'float')\n",
    "        radius = ((yy-ycent_pix)**2 + (xx-xcent_pix)**2)**0.5\n",
    "\n",
    "        # Select pixels within the aperture radius\n",
    "        mask = radius <= aperture_rad_pix\n",
    "\n",
    "        # Make a masked cube\n",
    "        maskedcube = cmin.with_mask(mask)\n",
    "\n",
    "        # Pixels in aperture\n",
    "        pix_in_ap = np.count_nonzero(mask == 1)\n",
    "\n",
    "        # Extract the spectrum from only the circular aperture - use sum\n",
    "        spectrum = maskedcube.sum(axis = (1,2))\n",
    "\n",
    "        # Extract the noise spectrum for the source\n",
    "        noisespectrum = maskedcube.std(axis = (1,2))\n",
    "\n",
    "        # Measure a spectrum from the background - Use an annulus around the source\n",
    "        # NOTE: Hardcoded values in for annulus size - improve\n",
    "\n",
    "        # Select pixels within an annulus\n",
    "        an_mask = (radius > aperture_rad_pix + 1) & (radius <= aperture_rad_pix + 2)\n",
    "\n",
    "        # Make a masked cube\n",
    "        an_maskedcube = cmin.with_mask(an_mask)\n",
    "\n",
    "        # Extract the background spectrum from only the annulus\n",
    "        bkg_spectrum = an_maskedcube.median(axis = (1,2))\n",
    "\n",
    "        # Background corrected spectrum - annulus\n",
    "        corr_sp = spectrum - (bkg_spectrum * pix_in_ap)\n",
    "\n",
    "        # Try measuring a spectrum from the background -> Use a box away from source.\n",
    "        # NOTE: Hardcoded values in for box region - improve\n",
    "\n",
    "        bkgcube = cmin[:, 13:16, 13:16]#[: , 1:3, 10:13]\n",
    "        bkgbox_spectrum = bkgcube.median(axis = (1,2))\n",
    "        bkg_img = bkgcube.median(axis = 0)\n",
    "\n",
    "        # Background corrected spectrum - box\n",
    "        corr_sp_box = spectrum - (bkgbox_spectrum * pix_in_ap)\n",
    "                \n",
    "        #-----------------------------------------------------               \n",
    "        # Plot the spectrum extracted from circular aperture via: a sum extraction\n",
    "        \n",
    "        plt.figure(figsize = (10,5))\n",
    "\n",
    "        plt.plot(maskedcube.spectral_axis.value, spectrum.value,\n",
    "                 label = 'Source')\n",
    "        plt.plot(maskedcube.spectral_axis.value, corr_sp.value,\n",
    "                 label = 'Bkg Corr')\n",
    "        plt.plot(maskedcube.spectral_axis.value, corr_sp_box.value,\n",
    "                 label = 'Bkg Corr box')\n",
    "\n",
    "        plt.xlabel('Wavelength (microns)')\n",
    "        plt.ylabel(spectrum.unit)\n",
    "\n",
    "        plt.gcf().text(0.5, 0.85, name, fontsize = 14, ha = 'center')\n",
    "\n",
    "        plt.gcf().text(0.5, 0.80, radec_lst[count_s].to_string('decimal'),\n",
    "                       ha = 'center', fontsize=14)\n",
    "\n",
    "        plt.legend(frameon = False, fontsize = 'medium')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        #-----------------------------------------------------  \n",
    "        # Convert flux from erg / (Angstrom cm2 s) to Jy \n",
    "        \n",
    "        spectrumJy = spectrum.to(\n",
    "            u.Jy, equivalencies = u.spectral_density(maskedcube.spectral_axis))\n",
    "\n",
    "        corr_sp_Jy = corr_sp.to(\n",
    "            u.Jy, equivalencies = u.spectral_density(maskedcube.spectral_axis))\n",
    "\n",
    "        corr_sp_box_Jy = corr_sp_box.to(\n",
    "            u.Jy, equivalencies= u.spectral_density(maskedcube.spectral_axis))\n",
    "\n",
    "        noiseSp_Jy = noisespectrum.to(\n",
    "            u.Jy, equivalencies = u.spectral_density(maskedcube.spectral_axis))\n",
    "\n",
    "        #-----------------------------------------------------\n",
    "        # Save each extracted spectrum to a file\n",
    "\n",
    "        # Set an output name\n",
    "        spec_outname = name + \"_\" + str(count_s) + \"_\" + \"spec\"\n",
    "\n",
    "        # Make output table \n",
    "        specdata_tab = Table([maskedcube.spectral_axis, corr_sp_Jy, noiseSp_Jy,\n",
    "                              spectrumJy, corr_sp_box_Jy],\n",
    "                             names=['wave_mum', 'cspec_Jy', 'err_fl_Jy',\n",
    "                                    'spec_Jy', 'cSpec_box_Jy'])\n",
    "\n",
    "        # Write the file\n",
    "        # ascii.write(specdata_tab, outdir_spectra + spec_outname +\".csv\",\n",
    "        #             format = 'csv', overwrite = True)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Do aperture photometry on the sources - Only if using sum of image\n",
    "\n",
    "    # Take list of star positions from DAOFIND use this to define an aperture\n",
    "    if len(sources) == 2:                         # To overcome in array order \n",
    "        sources = vstack([sources, sources])             \n",
    "        positions_pix = (sources['xcentroid'], sources['ycentroid'])\n",
    "    else:\n",
    "        positions_pix = (sources['xcentroid'], sources['ycentroid'])\n",
    "\n",
    "    apertures = CircularAperture(positions_pix, r = 2.)   # Aperture radius = 2 pixels\n",
    "    \n",
    "    #-----------------------------------------------------\n",
    "    # As a check to make sure all obvious point sources have been identified\n",
    "    # plot the cube with the NaN borders removed and overplot the apertures\n",
    "    # for the extracted sources\n",
    "    plt.figure()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cont_img.value, cmap='Greys', origin='lower')\n",
    "    apertures.plot(color='blue', lw=1.5, alpha=0.5)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cont_img.value, origin='lower')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:  \n",
    "    # Plot the cube with the NaN borders removed \n",
    "    plt.figure()\n",
    "    plt.imshow(cont_img.value, origin='lower')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make table of extracted sources\n",
    "source_extspec_tab = Table([name_val, source_val, ra_val, dec_val], \n",
    "                           names = (\"name\", \"source_no\", \"ra\", \"dec\"))\n",
    "print(source_extspec_tab)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data analysis - on the extracted spectra using `specutils`\n",
    "With the present lack of JWST flight data, we instead use the SWS spectra of an dusty AGB star, a cool M-star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Set the paths to the spectral data extracted from the datacube above\n",
    "dusty_AGB_spec_file = data_in_path + '63702662.txt'\n",
    "\n",
    "spectra_file = dusty_AGB_spec_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the spectra - as saved as text files & do some housekeeping\n",
    "data = ascii.read(spectra_file)\n",
    "\n",
    "if data.colnames[0] == 'col1':\n",
    "    data['col1'].name = 'wave_mum'\n",
    "    data['col2'].name = 'cspec_Jy'            \n",
    "    data['col3'].name = 'err_fl_Jy'         \n",
    "\n",
    "wav = data['wave_mum'] * u.micron  # Wavelength: microns\n",
    "fl = data['cspec_Jy'] * u.Jy       # Fnu:  Jy\n",
    "efl = data['err_fl_Jy'] * u.Jy     # Error flux: Jy\n",
    "\n",
    "# Make a 1D spectrum object\n",
    "spec = Spectrum1D(spectral_axis = wav, flux = fl, uncertainty = StdDevUncertainty(efl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Reading in a spectra comprised of multiple spectral components this file may have a spectral order column. In\n",
    "many instances these orders are not correctly stitched together due to issues with background and flux calibration. A\n",
    "spectral file with an order column that can read into the `Spectrum1D` is necessary to do corrections and scaling on\n",
    "each segment individually to fix the jumps between the spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a 5 pixel boxcar smoothing to the spectrum\n",
    "spec_bsmooth = box_smooth(spec, width = 5)   \n",
    "\n",
    "# Plot the spectrum & smoothed spectrum to inspect features \n",
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(spec.spectral_axis, spec.flux, label = 'Source')\n",
    "plt.plot(spec.spectral_axis, spec_bsmooth.flux, label = 'Smoothed')\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a continuum - find the best-fitting template (stellar photosphere model or blackbody)\n",
    "\n",
    "**Note** - Would idealy like to fit the photosphere with a set of Phoenix Models - but cant get that to work.\n",
    "I think `template_comparison` may be a good function here to work with the Phoenix Models which have been setup to\n",
    "interface with `pysynphot`.\n",
    "\n",
    "For now switching to a blackbody.\n",
    "\n",
    "- For AGB stars with a photosphere component fit a stellar photosphere model or a blackbody to short wavelength end of\n",
    "the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbody_Fnu(lam, T, A):\n",
    "    \"\"\" Blackbody as a function of wavelength (um) and temperature (K).\n",
    "        Function returns the Planck function in f_nu units\n",
    "        # [Y Jy] = 1.0E+23 * [X erg/cm^2/s/Hz] = 10E+26  [X Watts/m^2/Hz]\n",
    "    \"\"\"\n",
    "    from scipy.constants import h, k, c\n",
    "    lam = 1e-6 * lam                                              # convert to metres\n",
    "    bb_nu = 2*h*c / (lam**3 * (np.exp(h*c / (lam*k*T)) - 1))      # units of W/m^2/Hz/Steradian ; f_nu units\n",
    "    return A * bb_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to fit to a small wavelength range at the start of the spectra\n",
    "phot_fit_region = [3.0, 9.4]  # Microns\n",
    "\n",
    "# Trim the specrum to the region showing a stellar photosphere\n",
    "sub_region_phot = SpectralRegion([(phot_fit_region[0], phot_fit_region[1])] * u.micron)\n",
    "sub_spectrum_phot = extract_region(spec, sub_region_phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit BB to the data\n",
    "def phot_fn(wa, T1, A):\n",
    "    return blackbody_Fnu(wa, T1, A) \n",
    "\n",
    "popt, pcov = curve_fit(phot_fn, sub_spectrum_phot.spectral_axis.value,\n",
    "                       sub_spectrum_phot.flux.value, p0=(3000, 10000),\n",
    "                       sigma=sub_spectrum_phot.uncertainty.quantity)\n",
    "\n",
    "# Get the best fitting parameter value and their 1 sigma errors\n",
    "best_t1, best_a1 = popt\n",
    "sigma_t1, sigma_a1 = np.sqrt(np.diag(pcov))\n",
    "\n",
    "ybest = blackbody_Fnu(spec.spectral_axis.value, best_t1, best_a1)\n",
    "\n",
    "print ('Parameters of best-fitting model:')\n",
    "print ('T1 = %.2f +/- %.2f' % (best_t1, sigma_t1))\n",
    "\n",
    "degrees_of_freedom = len(sub_spectrum_phot.spectral_axis.value) - 2\n",
    "\n",
    "resid = (sub_spectrum_phot.flux.value - phot_fn(sub_spectrum_phot.spectral_axis.value, *popt)) \\\n",
    "        / sub_spectrum_phot.uncertainty.quantity\n",
    "\n",
    "chisq = np.dot(resid, resid)\n",
    "\n",
    "print ('nchi2 %.2f' % (chisq.value / degrees_of_freedom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spectrum & the model fit to the short wavelength region of the data.\n",
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(spec.spectral_axis, spec.flux, label = 'Source')\n",
    "plt.plot(spec.spectral_axis, ybest, label = 'BB')\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"Spectrum with blackbody fit\")\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Now subtract the BB and plot the underlying dust continuum\n",
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(spec.spectral_axis, spec.flux.value - ybest, label = 'Dust spectra')\n",
    "plt.axhline(0, color='r', linestyle = 'dashdot', alpha=0.5)\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"Continuum-subtracted spectrum\")\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now have the dust continuum want to look for features and measure their properties.\n",
    "\n",
    "Want to find:\n",
    "- Equivalent width\n",
    "- Equivalent flux\n",
    "- Optical depth\n",
    "- Centroids = wavelength with half the flux on either side\n",
    "\n",
    "#### As an example lets focus on the amorphous silicate 10 micron region.\n",
    "\n",
    "**Method - used repeatedly**\n",
    "\n",
    "- Fit a spline to the photosphere continuum subtracted spectra excluding the feature in this fit.\n",
    "- Trim the spectra to that wavelength region as the spline is now a different size to the full wavelength range of the\n",
    "spectra.\n",
    "- Make a continuum subtracted and and continuum normalised spectra.\n",
    "- Convert the units of the flux from Jy to W/m^2/wavelength for nice units post line integration. \n",
    "- Determine the feature line flux in units of W/m^2 and the feature centroid. Use continuum subtracted spectra.\n",
    "- Determine the feature equivalent width. Use continuum normalised spectra.\n",
    "- Make sure errors have been propagated correctly.\n",
    "- Store these results in a table \n",
    "- Several molecular and dust features are normally present in the spectra. Repeat for each feature.\n",
    "\n",
    "**Note**\n",
    "This seems like a long winded way to do this. Is there a simpler approach?\n",
    "\n",
    "> For instance a tool that takes four wavelengths, fits a line using the data from  lam0 to lam1 and lam2 to lam3, then\n",
    ">passes the continuum-subtracted  spectrum for line integration from lam1 to lam2 with error propagation is needed\n",
    ">several times for dust features. But with the current spectra1d framework this takes many steps to write manually and\n",
    ">is beyond tedious after doing this for 2 features let alone 20+.  Similar framework is also needed for the integrated\n",
    ">line centroid with uncertainty, and the extracted equivalent width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Fit a spline to the 10 micron feature to isolate it.\n",
    "\n",
    "bbsub_spectra = spec - ybest     # continuum subtracted spectra - Dust only\n",
    "\n",
    "# Fit a local continuum between the flux densities at: 8.0 - 8.1 & 14.9 - 15.0 microns\n",
    "# (i.e. excluding the line itself)\n",
    "\n",
    "sw_region = 8.0   #lam0\n",
    "sw_line = 8.1     #lam1\n",
    "lw_line = 14.9    #lam2\n",
    "lw_region = 15.0  #lam3\n",
    "\n",
    "# Zoom in on the line complex & extract\n",
    "line_reg_10 = SpectralRegion([(sw_region*u.um, lw_region*u.um)])\n",
    "line_spec = extract_region(bbsub_spectra, line_reg_10)\n",
    "\n",
    "# Fit a local continuum - exclude the actual dust feature when doing the fit\n",
    "\n",
    "lgl_fit = fit_generic_continuum(line_spec, \n",
    "                                exclude_regions = SpectralRegion([(sw_line*u.um, \n",
    "                                                                   lw_line*u.um)])) \n",
    "\n",
    "# Determine Y values of the line continuum\n",
    "line_y_continuum = lgl_fit(line_spec.spectral_axis) \n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Generate a continuum subtracted and continuum normalised spectra\n",
    "\n",
    "line_spec_norm =line_spec / line_y_continuum\n",
    "\n",
    "line_spec_consub = line_spec - line_y_continuum\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Plot the dust feature & continuum fit to the region\n",
    "\n",
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "plt.plot(line_spec.spectral_axis, line_spec.flux.value,\n",
    "         label = 'Dust spectra 10 micron region')\n",
    "\n",
    "plt.plot(line_spec.spectral_axis, line_y_continuum, label = 'Local continuum')\n",
    "\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"10$\\mu$m feature plus local continuum\")\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Plot the continuum subtracted 10 micron feature\n",
    "\n",
    "plt.figure(figsize = (8,4))\n",
    "\n",
    "plt.plot(line_spec_consub.spectral_axis, line_spec_consub.flux,\n",
    "         label = 'continuum subtracted')\n",
    "\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"Continuum subtracted 10$\\mu$m feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Calculate the Line flux; Line Centroid; Equivalent width\n",
    "# NOTE: Where are errors computed with these functions?\n",
    "\n",
    "line_centroid = centroid(line_spec_consub, SpectralRegion(sw_line*u.um, lw_line*u.um))\n",
    "\n",
    "line_flux_val = line_flux(line_spec_consub, SpectralRegion(sw_line*u.um, lw_line*u.um))\n",
    "\n",
    "equivalent_width_val = equivalent_width(line_spec_norm)\n",
    "\n",
    "# Hack to convert the line flux value into more conventional units\n",
    "# Necessary as spectra has mixed units: f_nu+lambda\n",
    "line_flux_val = (line_flux_val * u.micron).to(u.W * u.m**-2 * u.micron,\n",
    "                                              u.spectral_density(line_centroid)) / u.micron\n",
    "\n",
    "print(\"Line_centroid: {:.6} \".format(line_centroid))\n",
    "print(\"Integrated line_flux: {:.6} \".format(line_flux_val))\n",
    "print(\"Equivalent width: {:.6} \".format(equivalent_width_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Developer note** The hack in the cell above is necessary, as the line flux computed by `specutils` would return\n",
    "units of Jy micron and it is hard to convert this into conventional units within the current `specutils` framework.\n",
    "Line flux units should be in units of in W/m^2. Implementing a simple way to convert the flux and associate error to\n",
    "other units when dealing with a 1d spectal object with \"mixed\" spectral x and y axis units seems necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optical depth of the 10 micron feature\n",
    "tau = -(np.log(line_spec.flux.value / line_y_continuum.value))\n",
    "\n",
    "optdepth_spec = Spectrum1D(spectral_axis = line_spec.spectral_axis,\n",
    "                           flux = tau*(u.Jy/u.Jy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Developer note** Trying to put optical depth into a Spectrum1D object results in an error as no units.\n",
    "But the optical depth is unit-less - using (u.Jy/u.Jy) as work arround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optical depth of the 10 micron region vs wavelength\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(optdepth_spec.spectral_axis, optdepth_spec.flux)\n",
    "plt.xlabel(\"Wavelength ({:latex})\".format(spec.spectral_axis.unit))\n",
    "plt.ylabel('Tau') \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** At this point repeat *all* the steps above to isolate solid-state features e.g. for the forsterite feature at\n",
    "at approx 13.3 microns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now try looking for low crystalline silicate features  at 23, 28, 33 microns in the spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbsub_spectra = spec - ybest  # photosphere continuum subtracted spectra\n",
    "\n",
    "spline_points = [20.0, 21.3, 22.0, 24.4, 25.5, 33.8, 35.9] * u.micron  \n",
    "\n",
    "fluxc_resample = SplineInterpolatedResampler()\n",
    "\n",
    "#Â Generate a spline fit to the dust continuum\n",
    "spline_spec = fluxc_resample(bbsub_spectra, spline_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the underlying dust continuum and spline fit\n",
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(bbsub_spectra.spectral_axis, bbsub_spectra.flux.value, label = 'Dust spectra')\n",
    "plt.plot(spline_spec.spectral_axis, spline_spec.flux.value, label = 'Spline spectra')\n",
    "\n",
    "plt.axhline(0, color='r', linestyle='dashdot', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"Continuum-subtracted spectrum with spline\")\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot the underlying dust continuum and spline fit\n",
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(bbsub_spectra.spectral_axis, bbsub_spectra.flux.value, label = 'Dust spectra')\n",
    "plt.plot(spline_spec.spectral_axis, spline_spec.flux.value, label = 'Spline spectra')\n",
    "\n",
    "plt.xlim(spline_points[0].value, spline_points[-1].value)\n",
    "\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel(\"Flux ({:latex})\".format(spec.flux.unit))\n",
    "plt.title(\"Zoom of continuum-subtracted spectrum with spline\")\n",
    "plt.legend(frameon = False, fontsize = 'medium')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Developer note** By fitting a spline to a sub region the spectral shapes are no longer the same.\n",
    "` bbsub_spectra.flux.value - spline_spec.flux.value` now brakes. Would need to trim the spectrum to the spline size to\n",
    "start looking closely for low contrast dust features and again measure their properties (see above). Some  wrapper to\n",
    "stop repeating the same steps over and over would be nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [PampelMuse](https://gitlab.gwdg.de/skamann/pampelmuse)\n",
    "- [CASA](https://casa.nrao.edu/Release3.4.0/docs/UserMan/UserManse41.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this notebook\n",
    "**Author:** Olivia Jones, Project Scientist, UK ATC.\n",
    "**Updated On:** 2020-08-11\n",
    "**Later Updated On:** 2021-May-10 by B. Sargent, STScI Scientist, Space Telescope Science Institute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
